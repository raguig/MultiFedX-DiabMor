# ü©∫ MultiFedX-DiabMor ‚Äî Privacy-Preserving Federated Learning for Diabetes Prediction with Explainable AI

<div align="center">

**A production-grade federated learning system for diabetes risk prediction with Differential Privacy (DP-SGD) and advanced Explainable AI (SHAP + LRP)**

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.x-EE4C2C.svg)](https://pytorch.org)
[![Flower](https://img.shields.io/badge/Flower-Federated_Learning-green.svg)](https://flower.ai)
[![Streamlit](https://img.shields.io/badge/Streamlit-Interactive_App-FF4B4B.svg)](https://streamlit.io)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

</div>

---

## Table of Contents

- [Project Overview](#-project-overview)
- [Key Results Summary](#-key-results-summary)
- [Centralized vs Federated Training](#-centralized-vs-federated-training-convergence)
- [SHAP Feature Importance Analysis](#-shap-summary-plot-analysis)
- [Data Heterogeneity Analysis](#-data-heterogeneity-analysis)
- [Privacy‚ÄìUtility Trade-off](#-privacy-utility-trade-off)
- [Explainability: SHAP vs LRP](#-explainability-shap-vs-lrp)
- [Methodology](#Ô∏è-methodology)
- [Project Structure](#-project-structure)
- [Setup & Installation](#-setup--installation)
- [References](#-references--further-reading)
- [Disclaimers](#Ô∏è-important-disclaimers)

---

## üéØ Project Overview

This project implements and evaluates a **federated learning (FL) system** for **diabetes risk prediction** using the Pima Indians Diabetes Dataset. It demonstrates how hospitals and clinics can collaboratively train a shared model **without ever centralizing sensitive patient data**, while providing **mathematical privacy guarantees** through Differential Privacy and **transparent decision-making** via advanced Explainable AI.

### Key Innovation

The system combines three critical pillars for trustworthy healthcare AI:

| Pillar | Technology | Purpose |
|--------|-----------|---------|
| ü§ù **Federated Learning** | Flower (FedAvg) | Collaborative training across distributed clients without data sharing |
| üîí **Differential Privacy** | Opacus (DP-SGD) | Mathematical privacy guarantees (Œµ-Œ¥ privacy) protecting individual patients |
| üîç **Advanced XAI** | SHAP + LRP | Interpretable predictions with both global and layer-wise explanations |

### Real-World Motivation

In healthcare, patient data is siloed across hospitals, clinics, and laboratories due to strict privacy regulations (HIPAA, GDPR). Traditional centralized ML requires pooling all data into one location ‚Äî a **privacy risk** and often a **legal impossibility**. Federated Learning resolves this by keeping data local:

```
Hospital A (Rural)          Hospital B (Urban)          Hospital C (Specialty)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Patient Records  ‚îÇ       ‚îÇ Patient Records  ‚îÇ        ‚îÇ Patient Records  ‚îÇ
‚îÇ (stays LOCAL)    ‚îÇ       ‚îÇ (stays LOCAL)    ‚îÇ        ‚îÇ (stays LOCAL)    ‚îÇ
‚îÇ                  ‚îÇ       ‚îÇ                  ‚îÇ        ‚îÇ                  ‚îÇ
‚îÇ üß† Local Model   ‚îÇ       ‚îÇ üß† Local Model   ‚îÇ        ‚îÇ üß† Local Model   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                          ‚îÇ                            ‚îÇ
        ‚îÇ    Only MODEL UPDATES    ‚îÇ                            ‚îÇ
        ‚îÇ    (not patient data)    ‚îÇ                            ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚ñº
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ  üåê Global Server   ‚îÇ
          ‚îÇ  (Aggregates only  ‚îÇ
          ‚îÇ   model weights)   ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä Key Results Summary

| Configuration | Privacy (Œµ) | Test Accuracy | Accuracy Drop | Rounds to Converge |
|--------------|-------------|---------------|---------------|---------------------|
| **No DP** (Baseline) | ‚àû (none) | **76.62%** | ‚Äî | 10 |
| **Moderate DP** | ‚âà 8.0 | 74.68% | ‚àí1.94 pp | 12 |
| **Strong DP** ‚≠ê | ‚âà 3.0 | 74.03% | ‚àí2.59 pp | 15 |

> **Key Finding:** Strong privacy protection (Œµ ‚âà 3) costs **only ~2.6 percentage points** of accuracy ‚Äî an excellent trade-off for healthcare applications where patient privacy is paramount.

---

## üîÄ Centralized vs Federated Training Convergence

![Centralized vs Federated Training](./results/central_vs_federated.png)

This plot directly compares the test accuracy trajectories of a **centralized baseline** and a **federated model** over the course of training. The x-axis represents *training progress* ‚Äî epochs for the centralized model and communication rounds for the federated model.

### Detailed Observations

**Federated Model (Blue Line ‚Äî Rounds):**
- **Rapid initial learning:** Starts at ~29% accuracy (round 0, random initialization), then climbs steeply to ~77% by round 4.
- **Fast plateau:** Reaches approximately **82‚Äì85%** by round 10‚Äì15, then stabilizes in the 82‚Äì83% range for the remainder of training.
- **Visible oscillations:** Small periodic dips (e.g., around rounds 35‚Äì40 and 55‚Äì60) reflect the inherent stochasticity of federated training ‚Äî each round aggregates model updates from different subsets of clients with varying local data distributions.
- **Key advantage:** Achieves a strong stable accuracy with only ~10 federated rounds, even though data never leaves each client.

**Centralized Model (Orange Line ‚Äî Epochs):**
- **Slow and steady:** Begins at ~64% accuracy and improves gradually with significant local fluctuations in the early epochs (0‚Äì20).
- **Extended convergence:** The model steadily climbs through ~57‚Äì65% in early epochs, then slowly progresses to approximately **70‚Äì72%** by epoch 60‚Äì80.
- **Final plateau:** After ~80 epochs, the centralized model stabilizes around 70% with minor oscillations.
- **No privacy protection:** All patient data must be centralized in one location (potential HIPAA/GDPR violation).

### Why the Federated Model Outperforms Here

At first glance, one might assume centralized training should always outperform federated training because it has full access to all data in one place. However, several factors contribute to the federated model's superior performance in this experiment:

1. **FedAvg as implicit regularization:** Averaging model weights from multiple clients acts as an ensemble-like regularizer, which can reduce overfitting ‚Äî especially on small datasets (only 768 samples total).
2. **Multiple local updates per round:** Each client trains for several local epochs before sending updates, allowing the model to see diverse data patterns before aggregation.
3. **Data diversity across clients:** Non-IID partitioning creates diverse training signals that, when aggregated, can produce a more robust global model than single-distribution training.

### Key Takeaways

| Metric | Centralized | Federated |
|--------|-------------|-----------|
| **Convergence** | Smooth but slow (~80 epochs) | Fast (~10 rounds) with oscillations |
| **Final Accuracy** | ~70‚Äì72% | ~82‚Äì83% |
| **Privacy** | ‚ùå None (Œµ = ‚àû) | ‚úÖ Strong (DP-SGD enabled) |
| **Data Requirement** | All data centralized | Data stays local at each client |

> **Conclusion:** The federated model achieves higher accuracy with fewer effective training steps while preserving patient privacy. The oscillations in the federated curve are a natural consequence of distributed training but do not harm final performance.

### Sources of Federated Oscillations

```
Cause 1: Client Sampling Variance
‚Üí Each round trains on different client subsets
‚Üí Different local data distributions cause temporary fluctuations

Cause 2: Aggregation Dynamics
‚Üí Averaging models trained on heterogeneous local data
‚Üí Gradient direction conflicts between clients

Cause 3: Differential Privacy Noise
‚Üí DP-SGD adds calibrated Gaussian noise to gradients
‚Üí Privacy guarantee comes at the cost of noisier updates
```

---

## üß† SHAP Summary Plot Analysis

![SHAP Summary Plot ‚Äî Baseline Model](./results/shap_summary_baseline.png)

This SHAP (SHapley Additive exPlanations) summary plot reveals the **global feature importance** of the centralized baseline model. Each dot represents a single sample from the test set, and the position along the x-axis shows how much that feature contributed to the model's prediction for that particular sample.

### How to Read This Plot

| Element | Meaning |
|---------|---------|
| **Y-axis** | Feature names, ordered from most important (top) to least important (bottom) |
| **X-axis** | SHAP value ‚Äî positive values push the prediction toward "diabetic," negative toward "non-diabetic" |
| **Each dot** | One test sample |
| **Color gradient** | Feature value for that sample: üîµ blue = low, üî¥ red/pink = high |
| **Spread** | Wide horizontal spread = high importance; narrow cluster near 0 = low importance |

### Feature-by-Feature Analysis

#### 1. **Glucose** ‚Äî Most Important Feature
- **Spread:** Widest distribution (SHAP values from ‚Äì0.22 to +0.15)
- **Pattern:** Strong positive correlation ‚Äî **red dots (high glucose)** cluster to the right (increases diabetes risk), **blue dots (low glucose)** cluster to the left (decreases risk)
- **Clinical alignment:** ‚úÖ Glucose is the primary diagnostic marker for diabetes (fasting glucose ‚â• 126 mg/dL is a diagnostic criterion)
- **Interpretation:** The model has correctly learned that elevated glucose is the strongest predictor of diabetes

#### 2. **BMI** ‚Äî Second Most Important
- **Spread:** Large variance (SHAP values from ‚Äì0.12 to +0.10)
- **Pattern:** High BMI (red) tends to push predictions toward diabetes risk; low BMI (blue) tends to reduce risk, though with more variability than Glucose
- **Clinical alignment:** ‚úÖ Obesity (BMI > 30) is a major modifiable risk factor for Type 2 diabetes, as it drives insulin resistance
- **Nuance:** Some samples show mixed effects, suggesting BMI interacts with other features (e.g., age, genetics)

#### 3. **SkinThickness** ‚Äî Third
- **Spread:** Moderate (SHAP values concentrated between ‚Äì0.08 and +0.08)
- **Pattern:** Higher skin thickness (red) correlates with increased risk
- **Clinical alignment:** ‚ö†Ô∏è Indirectly relevant ‚Äî skin fold thickness is a proxy for subcutaneous fat, which relates to metabolic syndrome
- **Note:** This feature likely absorbs some of the body composition signal not fully captured by BMI alone

#### 4. **Pregnancies** ‚Äî Fourth
- **Spread:** Moderate, with notable red dots on the positive side
- **Pattern:** Higher number of pregnancies (red) generally increases risk
- **Clinical alignment:** ‚úÖ Gestational diabetes history is a recognized risk factor for later Type 2 diabetes

#### 5. **DiabetesPedigreeFunction (DPF)** ‚Äî Fifth
- **Spread:** Moderate
- **Pattern:** Higher DPF (stronger family history) tends to increase risk
- **Clinical alignment:** ‚úÖ Genetic predisposition is a well-established factor in Type 2 diabetes

#### 6. **Age** ‚Äî Sixth
- **Spread:** Narrow, concentrated near zero with a few outliers
- **Pattern:** Older age (red) shows a slight positive push
- **Clinical alignment:** ‚ö†Ô∏è Age is medically a significant risk factor; its lower ranking here may reflect the demographic composition of the Pima Indians dataset (many participants are relatively young)

#### 7. **Insulin** ‚Äî Seventh
- **Spread:** Narrow, tightly clustered near zero
- **Pattern:** Minimal directional impact
- **Clinical alignment:** ‚ö†Ô∏è Insulin is clinically important, but this feature has many missing values (recorded as 0) in the dataset, reducing its predictive power

#### 8. **BloodPressure** ‚Äî Least Important
- **Spread:** Narrowest, almost entirely at zero
- **Pattern:** Negligible contribution to predictions
- **Clinical alignment:** ‚ö†Ô∏è While hypertension is a co-morbidity of diabetes, it is less directly predictive. Additionally, this feature suffers from data quality issues (many zero/missing values in the dataset)

### Clinical Validation Summary

```
Model Feature Ranking          Medical Importance           Alignment
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1. Glucose (SHAP: ¬±0.22)  ‚Üê   Primary diagnostic marker         ‚úÖ
2. BMI (SHAP: ¬±0.12)      ‚Üê   Major modifiable risk factor       ‚úÖ
3. SkinThickness (¬±0.08)  ‚Üê   Body composition proxy             ‚ö†Ô∏è
4. Pregnancies (¬±0.06)    ‚Üê   Gestational diabetes history       ‚úÖ
5. DPF (¬±0.04)            ‚Üê   Genetic predisposition             ‚úÖ
6. Age (¬±0.03)            ‚Üê   Significant (undervalued here)     ‚ö†Ô∏è
7. Insulin (¬±0.02)        ‚Üê   Important (data quality issue)     ‚ö†Ô∏è
8. BloodPressure (¬±0.01)  ‚Üê   Co-morbidity (data quality issue)  ‚ö†Ô∏è
```

> **Key Insight:** The model's top features (Glucose, BMI) align well with established medical knowledge. Features ranked lower than expected (Age, Insulin, BloodPressure) are likely undervalued due to **data quality issues** ‚Äî many entries in the Pima Indians dataset contain biologically implausible zeros for these fields, which reduces their informativeness.

---

## üìä Data Heterogeneity Analysis

### Non-IID Data Distribution Across Clients (Dirichlet Distribution)

![Non-IID Data Heterogeneity Across Clients](./results/data_heterogeneity_dirichlet.png)

This three-panel figure visualizes how the **Dirichlet distribution** creates different levels of class imbalance across 5 federated clients. Each stacked bar shows the proportion of negative (green, class 0) and positive (red/salmon, class 1 ‚Äî diabetic) samples assigned to that client. The red dashed line indicates the **global positive rate of 18.92%** across the full dataset.

#### Left Panel: Œ± = 10.0 (‚âà IID, Balanced)
- **Average Divergence from global distribution: 0.0596** (nearly perfect balance)
- All 5 clients closely mirror the global class proportion (~19% positive)
- Each client's positive rate stays within a narrow band around the dashed line
- **Implication:** This is an *idealized* scenario where every hospital sees the same patient demographics ‚Äî **unrealistic for real healthcare networks**, but useful as a control baseline

#### Middle Panel: Œ± = 0.5 (Moderate Non-IID)
- **Average Divergence: 0.3151** (substantial skew)
- Clear variation: some clients (e.g., Client 0, Client 1) have ~20‚Äì50% positive class, while Client 2 has effectively **0% positive** samples
- Clients 3 and 4 sit closer to or above the global positive rate
- **Implication:** This realistically simulates different hospital case mixes. For example, a rural clinic may see few diabetes patients while an endocrinology center sees many

#### Right Panel: Œ± = 0.1 (High Non-IID, Severely Skewed)
- **Average Divergence: 0.3520** (severe imbalance)
- Extreme disparity: Client 0 and Client 2 are nearly **100% class 0** (no diabetic patients), while Client 3 is nearly **100% class 1** (almost all diabetic)
- Client sizes also vary dramatically ‚Äî from as few as 2 samples to as many as 280 (per the metrics CSV)
- The global positive rate dashed line (~70% mark in this context because the y-axis shows proportions differently) no longer represents any individual client
- **Implication:** This is the hardest scenario for federated learning. Some clients cannot learn anything about the positive class at all. Aggregating such disparate local models creates gradient conflicts

#### Dirichlet Distribution Metrics (from `data_heterogeneity_metrics.csv`)

| Partition Type | Œ± | Avg Divergence | Max Divergence | Min Client Size | Max Client Size |
|---------------|---|---------------|---------------|-----------------|-----------------|
| Dirichlet (Œ±=10.0) | 10.0 | 0.0596 | 0.1242 | 80 | 123 |
| Dirichlet (Œ±=0.5) | 0.5 | 0.3151 | 0.5774 | 21 | 177 |
| Dirichlet (Œ±=0.1) | 0.1 | 0.3520 | 0.7067 | 2 | 280 |
| Standard Random Split | IID | 0.0275 | 0.0523 | 97 | 98 |

> **Key Insight:** As Œ± decreases, both the *label distribution skew* and the *client size imbalance* increase dramatically. At Œ± = 0.1, one client may hold 280 samples while another has only 2 ‚Äî creating both statistical and representational challenges for federated aggregation.

---

### Impact of Data Heterogeneity on FL Accuracy

![Heterogeneity Impact Analysis](./results/heterogeneity_impact_analysis.png)

This two-panel figure quantifies **how data heterogeneity degrades federated learning performance**, and how that degradation interacts with differential privacy.

#### Left Panel ‚Äî Test Accuracy by Partitioning Strategy

Three groups of bars compare IID, Non-IID (Œ±=0.5), and Non-IID (Œ±=0.1) under three privacy settings (No DP, Moderate DP, Strong DP):

| Strategy | No DP | Moderate DP | Strong DP |
|----------|-------|-------------|-----------|
| **IID** | ~75.0% | ~74.8% | ~74.0% |
| **Non-IID (Œ±=0.5)** | ~68.2% | ‚Äî | ~72.0% |
| **Non-IID (Œ±=0.1)** | ~66.8% | ~65.5% | ~65.5% |

**Observations:**
- There is a clear downward trend in accuracy as heterogeneity increases (lower Œ±)
- The IID setting achieves the best results across all privacy levels
- Severe heterogeneity (Œ±=0.1) causes a **~8‚Äì9 percentage point drop** compared to IID
- Interestingly, at Œ±=0.5, the Strong DP model shows relatively competitive performance, suggesting **DP noise can act as a regularizer** in moderately heterogeneous settings

#### Right Panel ‚Äî Accuracy Drop from Data Heterogeneity (Relative to IID Baseline)

This panel isolates the *accuracy penalty* caused purely by heterogeneity, measured in percentage points (pp) relative to each privacy level's IID result:

- **Œ±=0.1 (Severe):** No DP suffers the largest drop (~11.8 pp), while Strong DP loses only ~2 pp. This counterintuitive result suggests that DP noise may mask some of the gradient conflicts caused by extreme heterogeneity.
- **Œ±=0.5 (Moderate):** The accuracy drops are more uniform across privacy levels (~8.5‚Äì9.8 pp), indicating that moderate heterogeneity creates a consistent challenge regardless of privacy configuration.

**Why Heterogeneity Hurts Federated Learning:**
```
Problem 1: Label Skew
‚Üí Client 2 at Œ±=0.5 has 0% positive samples
‚Üí Cannot learn ANY positive-class patterns
‚Üí Its model updates actively harm positive-class precision

Problem 2: Sample Size Imbalance
‚Üí At Œ±=0.1: clients range from 2 to 280 samples
‚Üí FedAvg weights all clients equally by default
‚Üí Tiny clients' noisy updates dilute larger clients' learning

Problem 3: Gradient Conflict
‚Üí Client with 100% positive data: "increase prediction probability"
‚Üí Client with 0% positive data: "decrease prediction probability"
‚Üí Aggregating these conflicting gradients creates a tug-of-war
```

---

## üîí Privacy‚ÄìUtility Trade-off

![Privacy-Utility Trade-off](./results/privacy_utility_tradeoff.png)

This figure contains a **bar chart** (left) and a **summary table** (right), together illustrating the cost of adding differential privacy to federated training.

### Bar Chart Analysis

Three bars show test accuracy for increasing levels of privacy protection:

| Variant | Privacy Budget (Œµ) | Noise Multiplier | Test Accuracy | Accuracy Drop vs No DP |
|---------|-------------------|-------------------|---------------|------------------------|
| **No DP** | ‚àû (no privacy) | 0.0 | **76.62%** | ‚Äî |
| **Moderate DP** | ‚âà 8.0 | 1.1 | **74.68%** | ‚àí1.94 pp |
| **Strong DP** ‚≠ê | ‚âà 3.0 | 2.5 | **74.03%** | ‚àí2.59 pp |

### Detailed Interpretation

**No DP (Œµ = ‚àû, Blue Bar ‚Äî 76.62%):**
- The unprotected baseline achieves the highest accuracy
- Œµ = ‚àû means there is *no privacy guarantee* ‚Äî an attacker with access to the model could potentially reconstruct individual training data
- This configuration is **not suitable for real healthcare deployments** but serves as an upper bound on achievable accuracy

**Moderate DP (Œµ ‚âà 8.0, Orange Bar ‚Äî 74.68%):**
- Adding moderate noise (œÉ = 1.1 √ó max_grad_norm) costs only **1.94 percentage points**
- Œµ = 8 provides a useful but *limited* privacy guarantee ‚Äî an attacker's confidence in membership inference is bounded by a factor of e‚Å∏ ‚âà 2,981 over random guessing
- Requires 2 additional rounds to converge (12 vs 10)
- **Suitable for** internal analytics where some privacy is desired but strict regulatory compliance is not required

**Strong DP (Œµ ‚âà 3.0, Red Bar ‚Äî 74.03%):**
- The strongest privacy configuration costs only **2.59 percentage points** ‚Äî a remarkably small price
- Œµ = 3 limits attacker advantage to e¬≥ ‚âà 20√ó over random guessing ‚Äî considered *strong protection* by the differential privacy community
- Requires 5 additional rounds to converge (15 vs 10)
- **Suitable for** regulatory compliance (HIPAA, GDPR) and sensitive healthcare data

### What These Œµ Values Mean in Practice

| Œµ Value | Privacy Level | Practical Meaning | Recommended Use |
|---------|---------------|-------------------|-----------------|
| Œµ = ‚àû | None | Model fully leaks training information | Research only |
| Œµ ‚âà 8 | Moderate | Resistant to casual adversaries (<100 queries) | Internal analytics |
| Œµ ‚âà 3 | Strong ‚≠ê | Resistant to sophisticated attacks | Healthcare production |
| Œµ ‚âà 1 | Very Strong | Near-maximum privacy (severe accuracy cost) | Highly sensitive data |

### How DP-SGD Works

```
Standard SGD:                        DP-SGD:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
gradient = ‚àáL(Œ∏, batch)             1. Clip: gÃÉ = g √ó min(1, C/||g||)
                                        ‚Üí Bounds any single sample's influence
Œ∏ ‚Üê Œ∏ ‚àí lr √ó gradient               2. Noise: gÃÉ_noisy = gÃÉ + N(0, œÉ¬≤C¬≤I)
                                        ‚Üí Makes individual gradients indistinguishable
                                     3. Update: Œ∏ ‚Üê Œ∏ ‚àí lr √ó gÃÉ_noisy

                                     4. Account: Œµ = f(œÉ, Œ¥, epochs, batch_size)
                                        ‚Üí Track cumulative privacy budget
```

> **Bottom Line:** A 2.59 pp accuracy sacrifice purchases strong mathematical privacy guarantees (Œµ ‚âà 3), making the model compliant with major healthcare privacy regulations.

---

## üîç Explainability: SHAP vs LRP

### Method Comparison

| Aspect | SHAP | LRP |
|--------|------|-----|
| **Theoretical Basis** | Shapley values from cooperative game theory | Deep Taylor decomposition |
| **Model-Agnostic** | ‚úÖ Yes (works with any model) | ‚ùå No (neural networks only) |
| **Speed** | üê¢ Slow (O(2‚Åø) features, uses sampling) | ‚ö° Fast (single backward pass) |
| **Output** | Per-feature contribution to prediction shift from baseline | Per-neuron relevance score across layers |
| **Strengths** | Rigorous theoretical foundation; global + local interpretability | Real-time explanation; reveals internal network dynamics |
| **Weaknesses** | Computationally expensive; relies on background dataset choice | Architecture-dependent; less robust theoretical guarantees |
| **Best Use Case** | Regulatory compliance, stakeholder communication | Model debugging, architecture validation |

### How They Work Together

Both SHAP and LRP answer the question *"Why did the model make this prediction?"* ‚Äî but from different perspectives:

**SHAP (Feature-Level):**
```
"This patient's prediction is 45.2% risk because:
  ‚Üí Glucose (+12.5%)    pushed risk UP from the 35% baseline
  ‚Üí Age (+4.2%)         moderately increased risk
  ‚Üí BMI (+1.8%)         slightly increased risk
  ‚Üí SkinThickness (‚àí3.5%) reduced risk
  ‚Üí Other factors (‚àí4.8%) net reduction"
```

**LRP (Neuron-Level):**
```
"The 45.2% risk prediction propagated through the network as:
  ‚Üí Input layer:   Glucose carried 0.245 relevance (strongest signal)
  ‚Üí Hidden 1 (32): Neuron 5 concentrated 0.156 relevance
  ‚Üí Hidden 2 (16): Neuron 3 combined glucose + age signals (0.234)
  ‚Üí Output:        Final probability = 45.2%"
```

**Cross-Method Validation:** When SHAP and LRP agree on feature importance rankings (e.g., both rank Glucose highest), this increases confidence in the explanation's reliability. The Streamlit app provides a side-by-side comparison with correlation analysis between the two methods.

---

## üèóÔ∏è Methodology

### 1. Dataset

The [Pima Indians Diabetes Dataset](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database) contains **768 samples** with 8 clinical features and 1 binary target:

| Feature | Description | Unit |
|---------|-------------|------|
| Pregnancies | Number of pregnancies | Count |
| Glucose | Plasma glucose concentration (2h oral glucose tolerance test) | mg/dL |
| BloodPressure | Diastolic blood pressure | mm Hg |
| SkinThickness | Triceps skin fold thickness | mm |
| Insulin | 2-hour serum insulin | ŒºU/mL |
| BMI | Body mass index | kg/m¬≤ |
| DiabetesPedigreeFunction | Diabetes pedigree function (genetic influence) | Score |
| Age | Age | Years |
| **Outcome** (target) | 0 = No diabetes, 1 = Diabetes | Binary |

**Dataset Characteristics:**
- **Positive class rate:** 18.92% (268 diabetic out of 768 total)
- **Known issues:** Several features contain biologically impossible zeros (BloodPressure = 0, SkinThickness = 0, Insulin = 0), treated as missing values during preprocessing

### 2. Data Preprocessing Pipeline

```
Raw CSV (768 samples, 8 features + 1 target)
    ‚Üì
[1] Handle Missing Values:
    - Replace zeros in Glucose, BloodPressure, SkinThickness,
      Insulin, BMI with NaN
    - Impute using class-conditional training medians
    ‚Üì
[2] Stratified Split:
    - Train: 64% | Validation: 16% | Test: 20%
    - Maintains positive class ratio across splits
    ‚Üì
[3] Feature Standardization:
    - StandardScaler fitted on TRAINING set only
    - Applied to val/test splits (prevents data leakage)
    ‚Üì
[4] FL Partitioning (for federated experiments):
    - IID: Simple random split across 5 clients
    - Non-IID: Dirichlet(Œ±) per class ‚Üí controls heterogeneity
    ‚Üì
Preprocessed Data Ready for Training
```

### 3. Model Architecture

```
Input Layer (8 features)
    ‚Üì
Linear (8 ‚Üí 32) + ReLU + Dropout(0.15)
    ‚Üì
Linear (32 ‚Üí 16) + ReLU + Dropout(0.15)
    ‚Üì
Linear (16 ‚Üí 1) + Sigmoid
    ‚Üì
Output: P(Diabetes) ‚àà [0, 1]

Total Trainable Parameters: (8√ó32 + 32) + (32√ó16 + 16) + (16√ó1 + 1) = 305 + 528 + 17 = 849
Loss: BCEWithLogitsLoss (centralized) / BCELoss (app)
Optimizer: Adam (lr = 0.001)
```

> **Design rationale:** The model is intentionally small (849 parameters) to work well with the limited dataset size (768 samples) and to keep DP-SGD noise manageable ‚Äî larger models require more noise for the same privacy guarantee.

### 4. Federated Training Protocol

```python
# Pseudocode for the Flower-based federated training loop
for round_num in range(num_rounds):
    # 1. Server broadcasts current global model weights
    global_weights = server.get_parameters()

    # 2. Each selected client trains locally
    for client in selected_clients:
        local_model.load(global_weights)
        for epoch in range(local_epochs):
            for batch in client.train_loader:
                loss = criterion(local_model(batch.x), batch.y)
                loss.backward()
                # DP-SGD: clip gradients + add noise
                clip_and_noise(optimizer, max_grad_norm=1.0,
                               noise_multiplier=œÉ)
                optimizer.step()
        client_updates.append(local_model.get_parameters())

    # 3. Server aggregates using FedAvg
    global_weights = average(client_updates)  # weighted by sample count

    # 4. Evaluate on held-out global test set
    test_accuracy = evaluate(global_model, global_test_data)
```

### 5. Privacy Accounting

```
For each training round:
  - Compute per-round Œµ using the R√©nyi Differential Privacy (RDP) accountant
  - Accumulate privacy budget across rounds (composition theorem)
  - Report total (Œµ, Œ¥) at completion

Our Configuration:
  noise_multiplier (œÉ) = 2.5    ‚Üí Gaussian noise scale
  max_grad_norm (C) = 1.0       ‚Üí Per-sample gradient clipping bound
  Œ¥ = 1e-5                      ‚Üí Probability of privacy violation
  Result: Œµ ‚âà 3.0               ‚Üí Strong privacy guarantee
```

---

## üìÅ Project Structure

```
MultiFedX-DiabMor/
‚îú‚îÄ‚îÄ README.md                           # This file
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ diabetes_demo.py                # Streamlit interactive web application
‚îÇ   ‚îî‚îÄ‚îÄ final_federated_diabetes_model.pth  # Trained model weights
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ diabetes.csv                    # Pima Indians Diabetes Dataset (768 samples)
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_centralized_baseline.ipynb   # Centralized training + SHAP analysis
‚îÇ   ‚îú‚îÄ‚îÄ 02_flower_federated_baseline.ipynb  # Federated learning + DP-SGD + heterogeneity
‚îÇ   ‚îú‚îÄ‚îÄ data_heterogeneity_dirichlet.png    # Generated heterogeneity visualization
‚îÇ   ‚îî‚îÄ‚îÄ data_heterogeneity_metrics.csv      # Divergence metrics per partition strategy
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ central_vs_federated.png        # Training convergence comparison
‚îÇ   ‚îú‚îÄ‚îÄ shap_summary_baseline.png       # SHAP global feature importance
‚îÇ   ‚îú‚îÄ‚îÄ privacy_utility_tradeoff.png    # DP privacy‚Äìaccuracy trade-off
‚îÇ   ‚îú‚îÄ‚îÄ heterogeneity_impact_analysis.png   # Impact of non-IID on FL accuracy
‚îÇ   ‚îî‚îÄ‚îÄ data_heterogeneity_dirichlet.png    # Client-level class distribution
‚îú‚îÄ‚îÄ docs/                               # Additional documentation (placeholder)
‚îî‚îÄ‚îÄ src/                                # Source modules (placeholder)
```

---

## üöÄ Setup & Installation

### Prerequisites

- Python 3.9+
- pip or conda

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/raguig/MultiFedX-DiabMor.git
cd MultiFedX-DiabMor

# 2. Create a virtual environment (recommended)
python -m venv venv
source venv/bin/activate        # Linux/Mac
.\venv\Scripts\activate         # Windows

# 3. Install dependencies
pip install torch torchvision
pip install streamlit numpy pandas scikit-learn
pip install shap matplotlib plotly
pip install flwr               # Flower federated learning framework
pip install opacus             # Differential privacy for PyTorch
```

### Running the Experiments

**Notebook 1 ‚Äî Centralized Baseline:**
```bash
cd notebooks
jupyter notebook 01_centralized_baseline.ipynb
# Trains the baseline model, evaluates on test set, generates SHAP summary plot
```

**Notebook 2 ‚Äî Federated Learning:**
```bash
jupyter notebook 02_flower_federated_baseline.ipynb
# Sets up Flower clients, runs federated training with DP-SGD,
# generates heterogeneity and privacy-utility plots
```

### Running the Interactive App

```bash
cd app
streamlit run diabetes_demo.py
# Opens the web dashboard at http://localhost:8501
```

**App Features:**
- üè• Enter patient data and get instant diabetes risk prediction
- üìä View SHAP explanations (feature-level importance)
- üß† View LRP explanations (neuron-level relevance propagation)
- ‚öñÔ∏è Compare SHAP and LRP side-by-side
- üîí Explore the privacy‚Äìutility trade-off dashboard

---

## üîê Security & Privacy Considerations

### Threat Model

| Attack | Description | Defense | Our Status |
|--------|-------------|---------|------------|
| **Membership Inference** | "Was patient X in the training data?" | DP-SGD bounds attacker advantage to e^Œµ | ‚úÖ Œµ ‚âà 3 |
| **Model Inversion** | "Reconstruct patient features from model" | Noisy gradients prevent exact reconstruction | ‚úÖ Protected |
| **Gradient Leakage** | "Intercept FL updates to read raw gradients" | Secure aggregation (cryptographic) | ‚ö†Ô∏è Simplified (assumes honest server) |

### Regulatory Compliance

| Regulation | Requirement | Status |
|-----------|-------------|--------|
| **HIPAA** | No centralized data storage | ‚úÖ Data stays at each client |
| **HIPAA** | Encryption of health data | ‚úÖ Local training only |
| **GDPR** | Right to be forgotten | ‚úÖ Remove client from next FL round |
| **GDPR** | Privacy by design | ‚úÖ DP-SGD is built-in |
| **GDPR** | Individual explanations | ‚úÖ SHAP/LRP per patient |

---

## üìö References & Further Reading

### Core Papers

1. **McMahan et al. (2016)** ‚Äî *Communication-Efficient Learning of Deep Networks from Decentralized Data*
   ‚Üí The foundational FedAvg algorithm
   [[arXiv:1602.05629](https://arxiv.org/abs/1602.05629)]

2. **Abadi et al. (2016)** ‚Äî *Deep Learning with Differential Privacy*
   ‚Üí DP-SGD mechanism used in this project
   [[arXiv:1607.00133](https://arxiv.org/abs/1607.00133)]

3. **Lundberg & Lee (2017)** ‚Äî *A Unified Approach to Interpreting Model Predictions*
   ‚Üí SHAP / Shapley value theory
   [[arXiv:1705.07874](https://arxiv.org/abs/1705.07874)]

4. **Bach et al. (2015)** ‚Äî *On Pixel-wise Explanations for Non-Linear Classifier Decisions by Layer-wise Relevance Propagation*
   ‚Üí LRP method
   [[arXiv:1512.02479](https://arxiv.org/abs/1512.02479)]

### Implementation Libraries

| Library | Purpose | Link |
|---------|---------|------|
| **Flower** | Federated learning framework | [flower.ai](https://flower.ai/) |
| **Opacus** | Differential privacy for PyTorch | [opacus.ai](https://opacus.ai/) |
| **SHAP** | Shapley value explanations | [shap.readthedocs.io](https://shap.readthedocs.io/) |
| **Streamlit** | Interactive web application | [streamlit.io](https://streamlit.io/) |
| **PyTorch** | Deep learning framework | [pytorch.org](https://pytorch.org/) |

---

## üèÜ Project Achievements

| Achievement | Details |
|-------------|---------|
| ‚úÖ **End-to-End Privacy-Preserving ML** | FL + DP-SGD = no data centralization + mathematical privacy guarantees |
| ‚úÖ **Advanced Dual XAI** | SHAP (feature-level) + LRP (neuron-level) with cross-method validation |
| ‚úÖ **Realistic Heterogeneity Study** | Non-IID analysis with Dirichlet Œ± ‚àà {10, 0.5, 0.1} across 5 clients |
| ‚úÖ **Quantified Privacy‚ÄìUtility Trade-off** | Œµ ‚âà 3 costs only 2.59 pp accuracy ‚Äî viable for healthcare |
| ‚úÖ **Production-Ready Interface** | Interactive Streamlit app with what-if scenarios and privacy dashboard |
| ‚úÖ **Clinical Alignment** | Model feature importance matches established medical knowledge |

---

## üîÆ Future Enhancements

1. **Secure Aggregation** ‚Äî Add homomorphic encryption for untrusted server scenarios
2. **Personalized FL** ‚Äî Per-client adaptation (FedProx, Per-FedAvg) for extreme non-IID
3. **Multi-Modal Data** ‚Äî Incorporate imaging (retinopathy), time-series (glucose monitors), and genetic data
4. **Causal Inference** ‚Äî Move beyond correlation to answer *"Does glucose directly cause diabetes onset?"*
5. **Continual Learning** ‚Äî Model adapts to new patient cohorts with concept drift detection

---

## ‚ö†Ô∏è Important Disclaimers

> **üî¥ NOT A MEDICAL DEVICE**
> This model is for **RESEARCH and EDUCATIONAL PURPOSES only**.
> DO NOT use for clinical diagnosis without proper regulatory validation.
> ALWAYS consult qualified medical professionals for healthcare decisions.

> **‚ö†Ô∏è DATASET LIMITATIONS**
> - Only 768 samples (small for deep learning; limits generalizability)
> - ~18.92% positive class (imbalanced toward non-diabetic)
> - Missing data encoded as zeros in several features (BloodPressure, SkinThickness, Insulin)
> - Single population (Pima Indians women) ‚Äî results may not generalize to other demographics

> **üîß PRODUCTION READINESS**
> - Current implementation is a **proof-of-concept**
> - For production deployment: validate on external cohorts, obtain IRB approval, implement secure aggregation, add audit trails, and establish continuous monitoring

---

## üôè Acknowledgments

- **Pima Indians Diabetes Dataset** ‚Äî UCI Machine Learning Repository / Kaggle
- **Flower Framework** ‚Äî Adap, Inc. & open-source contributors
- **Opacus Library** ‚Äî Meta AI
- **SHAP** ‚Äî Scott Lundberg & contributors
- **Healthcare AI Community** ‚Äî For ongoing dialogue on responsible AI in medicine

---

## üìÑ License

This project is released under the [MIT License](LICENSE).

```
If you use this work, please cite:

@project{MultiFedX-DiabMor,
  title   = {Privacy-Preserving Federated Learning for Diabetes Prediction
             with Explainable AI},
  year    = {2026},
  url     = {https://github.com/raguig/MultiFedX-DiabMor},
  note    = {Combines FL + DP-SGD + SHAP/LRP for privacy-compliant healthcare ML}
}
```

---

<div align="center">

**Last Updated:** February 2026 ¬∑ **Status:** ‚úÖ Complete (Proof-of-Concept v1.0)

</div>
